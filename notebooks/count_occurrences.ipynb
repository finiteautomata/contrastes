{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting occurrences\n",
    "\n",
    "\n",
    "En esta notebook veremos cómo contar ocurrencias de los tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "tweets = json.load(open(\"data/tweets/buenosaires/001.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contamos las ocurrencias. No sumar el fd porque es lento ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "urls = r'(?:https?\\://t.co/[\\w]+)'\n",
    "\n",
    "\n",
    "tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "\n",
    "\n",
    "def mytokenize(text, only_alpha=True, remove_hashtags=True):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    if only_alpha:\n",
    "        tokens = [tk for tk in tokens if tk.isalpha()]\n",
    "    else:\n",
    "        tokens = [tk for tk in tokens if tk[0] != \"#\"] if remove_hashtags else tokens\n",
    "        tokens = [tk for tk in tokens if not re.match(urls, tk)]\n",
    "    tokens = [re.sub(r'(.)\\1\\1+', r'\\1\\1', tk) for tk in tokens]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.05 s, sys: 311 µs, total: 4.05 s\n",
      "Wall time: 4.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "Calculemos el freqdist \n",
    "\"\"\"\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "fd = nltk.FreqDist()\n",
    "users = defaultdict(set)\n",
    "\n",
    "for tweet in tweets:\n",
    "    text = tweet['text']\n",
    "    tokens = mytokenize(text)\n",
    "    for token in tokens:\n",
    "        users[token].add(tweet['user']['id'])\n",
    "        fd[token] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando todas estas opciones redujimos alrededor de 10K palabras (cerca de un 30%!) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de tokens = 404321\n",
      "Cantidad de tokens únicos = 27443\n"
     ]
    }
   ],
   "source": [
    "print(\"Cantidad de tokens = {}\".format(fd.N()))\n",
    "print(\"Cantidad de tokens únicos = {}\".format(fd.B()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\"bsas_occ\": fd, \"bsas-usuarios\": {k:len(v) for k, v in users.items()}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3071, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"bsas_occ\"] > 10].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haciendo todo Buenos Aires..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import json\n",
    "\n",
    "def prov_name(path):\n",
    "    return os.path.basename(os.path.normpath(path))\n",
    "\n",
    "provinces = [prov_name(path) for path in glob.glob(\"data/tweets/**/\")]\n",
    "\n",
    "\n",
    "path = \"data/tweets/buenosaires/\"\n",
    "\n",
    "jsons = glob.glob(os.path.join(path, \"*.json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing buenosaires\n",
      "file data/tweets/buenosaires/029.json\n",
      "file data/tweets/buenosaires/046.json\n",
      "file data/tweets/buenosaires/024.json\n",
      "file data/tweets/buenosaires/020.json\n",
      "file data/tweets/buenosaires/038.json\n",
      "file data/tweets/buenosaires/070.json\n",
      "file data/tweets/buenosaires/055.json\n",
      "file data/tweets/buenosaires/054.json\n",
      "file data/tweets/buenosaires/052.json\n",
      "file data/tweets/buenosaires/058.json\n",
      "file data/tweets/buenosaires/014.json\n",
      "file data/tweets/buenosaires/074.json\n",
      "file data/tweets/buenosaires/004.json\n",
      "file data/tweets/buenosaires/037.json\n",
      "file data/tweets/buenosaires/050.json\n",
      "file data/tweets/buenosaires/064.json\n",
      "file data/tweets/buenosaires/044.json\n",
      "file data/tweets/buenosaires/076.json\n",
      "file data/tweets/buenosaires/047.json\n",
      "file data/tweets/buenosaires/073.json\n",
      "file data/tweets/buenosaires/008.json\n",
      "file data/tweets/buenosaires/026.json\n",
      "file data/tweets/buenosaires/019.json\n",
      "file data/tweets/buenosaires/033.json\n",
      "file data/tweets/buenosaires/034.json\n",
      "file data/tweets/buenosaires/043.json\n",
      "file data/tweets/buenosaires/011.json\n",
      "file data/tweets/buenosaires/001.json\n",
      "file data/tweets/buenosaires/021.json\n",
      "file data/tweets/buenosaires/032.json\n",
      "file data/tweets/buenosaires/010.json\n",
      "file data/tweets/buenosaires/028.json\n",
      "file data/tweets/buenosaires/023.json\n",
      "file data/tweets/buenosaires/030.json\n",
      "file data/tweets/buenosaires/051.json\n",
      "file data/tweets/buenosaires/041.json\n",
      "file data/tweets/buenosaires/053.json\n",
      "file data/tweets/buenosaires/066.json\n",
      "file data/tweets/buenosaires/036.json\n",
      "file data/tweets/buenosaires/061.json\n",
      "file data/tweets/buenosaires/048.json\n",
      "file data/tweets/buenosaires/075.json\n",
      "file data/tweets/buenosaires/049.json\n",
      "file data/tweets/buenosaires/069.json\n",
      "file data/tweets/buenosaires/018.json\n",
      "file data/tweets/buenosaires/009.json\n",
      "file data/tweets/buenosaires/017.json\n",
      "file data/tweets/buenosaires/022.json\n",
      "file data/tweets/buenosaires/003.json\n",
      "file data/tweets/buenosaires/060.json\n",
      "file data/tweets/buenosaires/015.json\n",
      "file data/tweets/buenosaires/056.json\n",
      "file data/tweets/buenosaires/042.json\n",
      "file data/tweets/buenosaires/031.json\n",
      "file data/tweets/buenosaires/068.json\n",
      "file data/tweets/buenosaires/035.json\n",
      "file data/tweets/buenosaires/025.json\n",
      "file data/tweets/buenosaires/063.json\n",
      "file data/tweets/buenosaires/040.json\n",
      "file data/tweets/buenosaires/005.json\n",
      "file data/tweets/buenosaires/013.json\n",
      "file data/tweets/buenosaires/039.json\n",
      "file data/tweets/buenosaires/065.json\n",
      "file data/tweets/buenosaires/002.json\n",
      "file data/tweets/buenosaires/062.json\n",
      "file data/tweets/buenosaires/045.json\n",
      "file data/tweets/buenosaires/027.json\n",
      "file data/tweets/buenosaires/007.json\n",
      "file data/tweets/buenosaires/059.json\n",
      "file data/tweets/buenosaires/057.json\n",
      "file data/tweets/buenosaires/067.json\n",
      "file data/tweets/buenosaires/072.json\n",
      "file data/tweets/buenosaires/016.json\n",
      "file data/tweets/buenosaires/071.json\n",
      "file data/tweets/buenosaires/006.json\n",
      "file data/tweets/buenosaires/012.json\n",
      "Done. Building dataframe...\n",
      "Done\n",
      "CPU times: user 3min 19s, sys: 7.29 s, total: 3min 26s\n",
      "Wall time: 7min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import tracemalloc\n",
    "import multiprocessing\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "tracemalloc.start()\n",
    "\n",
    "def get_fd(json_path):\n",
    "    print(\"file {}\".format(json_path))\n",
    "    tweets = json.load(open(json_path))\n",
    "    fd = nltk.FreqDist()\n",
    "    users = defaultdict(set)\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        text = tweet['text']\n",
    "        tokens = mytokenize(text)\n",
    "        for token in tokens:\n",
    "            fd[token] += 1\n",
    "            users[token].add(tweet['user']['id'])\n",
    "    return fd, users \n",
    "\n",
    "\n",
    "def get_province_df(province_name, jsons, no_workers=4):\n",
    "    print(\"Processing {}\".format(province_name))\n",
    "    pool = multiprocessing.Pool(no_workers, maxtasksperchild=1)\n",
    "\n",
    "    fds = pool.map(get_fd, jsons)\n",
    "\n",
    "    fd = nltk.FreqDist()\n",
    "    users = None \n",
    "\n",
    "    for (other_fd, users_freq) in fds:\n",
    "        fd += other_fd\n",
    "        if users is None:\n",
    "            users = users_freq\n",
    "        else:\n",
    "            # Tengo que mergear los dicts de aquellas existentes\n",
    "            for k in itertools.chain(users.keys(), users_freq.keys()):\n",
    "                users[k] = users[k].union(users_freq[k])\n",
    "\n",
    "    users_occurrences = {k:len(v) for k, v in users.items()}\n",
    "    \n",
    "    occurrences_column = \"{}_ocurrencias\".format(province_name)\n",
    "    users_column = \"{}_usuarios\".format(province_name)\n",
    "    \n",
    "    print(\"Done. Building dataframe...\")\n",
    "    df = pd.DataFrame({occurrences_column: fd, users_column: users_occurrences})\n",
    "    print(\"Done\")\n",
    "    return df\n",
    "\n",
    "df = get_province_df(\"buenosaires\", jsons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot3 = tracemalloc.take_snapshot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jmperez/.pyenv/versions/3.6.4/lib/python3.6/multiprocessing/connection.py:251: size=19.4 MiB, count=339766, average=60 B\n",
      "/home/jmperez/.pyenv/versions/3.6.4/envs/contrastes/lib/python3.6/site-packages/pandas/core/internals.py:4821: size=5309 KiB, count=2, average=2655 KiB\n",
      "/home/jmperez/.pyenv/versions/3.6.4/envs/contrastes/lib/python3.6/site-packages/pandas/core/common.py:384: size=2655 KiB, count=10, average=265 KiB\n",
      "<frozen importlib._bootstrap_external>:487: size=84.9 KiB, count=1041, average=83 B\n",
      "/home/jmperez/.pyenv/versions/3.6.4/lib/python3.6/threading.py:846: size=6936 B, count=6, average=1156 B\n",
      "/home/jmperez/.pyenv/versions/3.6.4/envs/contrastes/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py:432: size=3400 B, count=15, average=227 B\n",
      "/home/jmperez/.pyenv/versions/3.6.4/lib/python3.6/multiprocessing/pool.py:144: size=2748 B, count=11, average=250 B\n",
      "/home/jmperez/.pyenv/versions/3.6.4/lib/python3.6/tempfile.py:150: size=2600 B, count=2, average=1300 B\n",
      "/home/jmperez/.pyenv/versions/3.6.4/lib/python3.6/threading.py:884: size=2520 B, count=3, average=840 B\n",
      "/home/jmperez/.pyenv/versions/3.6.4/lib/python3.6/codeop.py:133: size=2422 B, count=18, average=135 B\n"
     ]
    }
   ],
   "source": [
    "for statistic in snapshot3.statistics('lineno', cumulative=True)[:10]:\n",
    "        print(statistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame de todas las provincias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando larioja\n",
      "Sumando freqdists\n",
      "Procesando santiago\n",
      "Sumando freqdists\n",
      "Procesando formosa\n",
      "Sumando freqdists\n",
      "Procesando buenosaires\n",
      "Sumando freqdists\n",
      "Procesando neuquen\n",
      "Sumando freqdists\n",
      "Procesando santacruz\n",
      "Sumando freqdists\n",
      "Procesando cordoba\n",
      "Sumando freqdists\n",
      "Procesando entrerios\n",
      "Sumando freqdists\n",
      "Procesando misiones\n",
      "Sumando freqdists\n",
      "Procesando tierradelfuego\n",
      "Sumando freqdists\n",
      "Procesando santafe\n",
      "Sumando freqdists\n",
      "Procesando sanjuan\n",
      "Sumando freqdists\n",
      "Procesando catamarca\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cfd = nltk.ConditionalFreqDist()\n",
    "\n",
    "data = {}\n",
    "\n",
    "for prov in provinces:\n",
    "    json_paths = [\"data/tweets/{}/00{}.json\".format(prov, i ) for i in range(1, 5)]\n",
    "    print(\"Procesando {}\".format(prov))\n",
    "    \n",
    "    pool = multiprocessing.Pool(4)\n",
    "    fds = pool.map(get_fd, json_paths)\n",
    "    \n",
    "    print(\"Sumando freqdists\")\n",
    "    \n",
    "    fd = nltk.FreqDist()\n",
    "\n",
    "    for ff in fds:\n",
    "        fd += ff\n",
    "        \n",
    "    data[prov] = fd\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
