{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geolocalización\n",
    "\n",
    "En esta notebook haremos un intento de geolocalización con los textos de los usuarios..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient('localhost', 27018)\n",
    "\n",
    "db = client['contrastes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('5ba53c9827a5141aaa383eb9'),\n",
       " 'created_at': 'Tue Nov 05 14:48:51 +0000 2013',\n",
       " 'id': 397737276736040960,\n",
       " 'place': None,\n",
       " 'provincia': 'larioja',\n",
       " 'text': 'Estoy tan asustada :(',\n",
       " 'tokens': ['estoy', 'tan', 'asustada'],\n",
       " 'user_id': 301800629}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.tweets.find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tenemos 56308 usuarios\n"
     ]
    }
   ],
   "source": [
    "user_ids = list(db.users.distinct('id'))\n",
    "print(\"Tenemos {} usuarios\".format(len(user_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hagamos lo siguiente:\n",
    "\n",
    "- Entrenemos con unigramas una regresión logística para \n",
    "- Luego probemos con los regionalismos\n",
    "\n",
    "Primero, partamos en train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>provincia</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>buenosaires</th>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>catamarca</th>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chaco</th>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chubut</th>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cordoba</th>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>corrientes</th>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entrerios</th>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>formosa</th>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jujuy</th>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lapampa</th>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>larioja</th>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mendoza</th>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>misiones</th>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neuquen</th>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rionegro</th>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salta</th>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sanjuan</th>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sanluis</th>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>santacruz</th>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>santafe</th>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>santiago</th>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tierradelfuego</th>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tucuman</th>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                text\n",
       "provincia           \n",
       "buenosaires      156\n",
       "catamarca        162\n",
       "chaco            159\n",
       "chubut           164\n",
       "cordoba          166\n",
       "corrientes       171\n",
       "entrerios        167\n",
       "formosa          151\n",
       "jujuy            189\n",
       "lapampa          164\n",
       "larioja          168\n",
       "mendoza          173\n",
       "misiones         162\n",
       "neuquen          167\n",
       "rionegro         147\n",
       "salta            156\n",
       "sanjuan          155\n",
       "sanluis          158\n",
       "santacruz        170\n",
       "santafe          156\n",
       "santiago         168\n",
       "tierradelfuego   170\n",
       "tucuman          151"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(2019)\n",
    "\n",
    "sample_user_ids = random.sample(user_ids, 5000)\n",
    "\n",
    "# Nos quedamos sólo con los campos que nos interesan\n",
    "users = list(db.users.find({\"id\": {\"$in\": sample_user_ids}}, {\"id\": 1, \"_id\": 0, \"text\": 1, \"provincia\": 1}))\n",
    "\n",
    "\n",
    "\n",
    "train_users, test_users = train_test_split(users)\n",
    "\n",
    "df_train = pd.DataFrame(train_users)\n",
    "df_train.set_index(\"id\", inplace=True)\n",
    "\n",
    "\n",
    "df_test = pd.DataFrame(test_users)\n",
    "df_test.set_index(\"id\", inplace=True)\n",
    "\n",
    "df_train.groupby(\"provincia\").count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Palabras precalculadas\n",
    "\n",
    "Carguemos antes las palabras que sabemos que ocurren una cantidad razonable de veces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 18s, sys: 2.72 s, total: 5min 21s\n",
      "Wall time: 5min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "vectorizer = CountVectorizer(\n",
    "    tokenizer=tokenizer.tokenize, stop_words=stopwords.words('spanish'),\n",
    "    min_df=0.0007, max_df=0.15, ngram_range=(1, 2),\n",
    ")\n",
    "\n",
    "vectorizer.fit(df_train[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario del vectorizador: 1375444 palabras\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulario del vectorizador: {} palabras\".format(len(vectorizer.vocabulary_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer.transform(df_train[\"text\"])\n",
    "X_test = vectorizer.transform(df_test[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "province_encoder = LabelEncoder()\n",
    "\n",
    "province_encoder.fit(df_train[\"provincia\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = province_encoder.transform(df_train[\"provincia\"].values)\n",
    "y_test = province_encoder.transform(df_test[\"provincia\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La reg. logística será un softmax, así que elijo `multi_class='multinomial'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(multi_class='multinomial', solver='saga')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmperez/.pyenv/versions/3.6.5/envs/contrastes/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18min 30s, sys: 744 ms, total: 18min 31s\n",
      "Wall time: 18min 31s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='saga',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.68 s, sys: 112 ms, total: 1.79 s\n",
      "Wall time: 1.79 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6493333333333333"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 544 ms, sys: 84 ms, total: 628 ms\n",
      "Wall time: 626 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3856"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "38% de accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando sólo \"regionalismos\" o LIW (Location Indicative Words)\n",
    "\n",
    "Usemos ahora nuestros \"features\". Es decir, probemos con porcentajes de las palabras encontradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>buenosaires_ocurrencias</th>\n",
       "      <th>buenosaires_usuarios</th>\n",
       "      <th>catamarca_ocurrencias</th>\n",
       "      <th>catamarca_usuarios</th>\n",
       "      <th>chaco_ocurrencias</th>\n",
       "      <th>chaco_usuarios</th>\n",
       "      <th>chubut_ocurrencias</th>\n",
       "      <th>chubut_usuarios</th>\n",
       "      <th>cordoba_ocurrencias</th>\n",
       "      <th>cordoba_usuarios</th>\n",
       "      <th>...</th>\n",
       "      <th>guaranitica_usuarios</th>\n",
       "      <th>noroeste_ocurrencias</th>\n",
       "      <th>fnorm_noroeste</th>\n",
       "      <th>noroeste_usuarios</th>\n",
       "      <th>fnorm_region_max</th>\n",
       "      <th>region_max</th>\n",
       "      <th>fnorm_region_min</th>\n",
       "      <th>region_min</th>\n",
       "      <th>region_sin_palabra</th>\n",
       "      <th>max_dif_region</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>palabra</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>chivil</th>\n",
       "      <td>1970</td>\n",
       "      <td>515</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>7.424642</td>\n",
       "      <td>litoral</td>\n",
       "      <td>0.009863</td>\n",
       "      <td>guaranitica</td>\n",
       "      <td>3</td>\n",
       "      <td>752.782165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ush</th>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>42</td>\n",
       "      <td>0.276578</td>\n",
       "      <td>18</td>\n",
       "      <td>18.695060</td>\n",
       "      <td>litoral</td>\n",
       "      <td>0.082872</td>\n",
       "      <td>cuyo</td>\n",
       "      <td>0</td>\n",
       "      <td>225.590394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poec</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>4.605158</td>\n",
       "      <td>litoral</td>\n",
       "      <td>0.016530</td>\n",
       "      <td>central</td>\n",
       "      <td>2</td>\n",
       "      <td>278.585597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>malpegue</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>31.491247</td>\n",
       "      <td>cuyo</td>\n",
       "      <td>0.033061</td>\n",
       "      <td>central</td>\n",
       "      <td>2</td>\n",
       "      <td>952.519835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aijue</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>705</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>21.560377</td>\n",
       "      <td>guaranitica</td>\n",
       "      <td>0.016574</td>\n",
       "      <td>cuyo</td>\n",
       "      <td>2</td>\n",
       "      <td>1300.828621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tolhuin</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.032926</td>\n",
       "      <td>5</td>\n",
       "      <td>11.059897</td>\n",
       "      <td>litoral</td>\n",
       "      <td>0.016574</td>\n",
       "      <td>cuyo</td>\n",
       "      <td>0</td>\n",
       "      <td>667.290306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vallerga</th>\n",
       "      <td>1932</td>\n",
       "      <td>436</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>7.262991</td>\n",
       "      <td>litoral</td>\n",
       "      <td>0.016530</td>\n",
       "      <td>central</td>\n",
       "      <td>3</td>\n",
       "      <td>439.369284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yarca</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.019756</td>\n",
       "      <td>3</td>\n",
       "      <td>16.408597</td>\n",
       "      <td>cuyo</td>\n",
       "      <td>0.007519</td>\n",
       "      <td>litoral</td>\n",
       "      <td>1</td>\n",
       "      <td>2182.393459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blv</th>\n",
       "      <td>268</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0.039511</td>\n",
       "      <td>4</td>\n",
       "      <td>85.561838</td>\n",
       "      <td>central</td>\n",
       "      <td>0.039452</td>\n",
       "      <td>guaranitica</td>\n",
       "      <td>0</td>\n",
       "      <td>2168.772148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>portho</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>12.530202</td>\n",
       "      <td>cuyo</td>\n",
       "      <td>0.018797</td>\n",
       "      <td>litoral</td>\n",
       "      <td>3</td>\n",
       "      <td>666.622002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          buenosaires_ocurrencias  buenosaires_usuarios  \\\n",
       "palabra                                                   \n",
       "chivil                       1970                   515   \n",
       "ush                            12                     6   \n",
       "poec                            5                     2   \n",
       "malpegue                        0                     0   \n",
       "aijue                           0                     0   \n",
       "tolhuin                         1                     1   \n",
       "vallerga                     1932                   436   \n",
       "yarca                           0                     0   \n",
       "blv                           268                    43   \n",
       "portho                          0                     0   \n",
       "\n",
       "          catamarca_ocurrencias  catamarca_usuarios  chaco_ocurrencias  \\\n",
       "palabra                                                                  \n",
       "chivil                        0                   0                  0   \n",
       "ush                           4                   2                  4   \n",
       "poec                          0                   0                  0   \n",
       "malpegue                      0                   0                  0   \n",
       "aijue                         0                   0                  1   \n",
       "tolhuin                       0                   0                  4   \n",
       "vallerga                      0                   0                  0   \n",
       "yarca                         2                   2                  0   \n",
       "blv                           0                   0                  0   \n",
       "portho                        0                   0                  0   \n",
       "\n",
       "          chaco_usuarios  chubut_ocurrencias  chubut_usuarios  \\\n",
       "palabra                                                         \n",
       "chivil                 0                   0                0   \n",
       "ush                    3                   4                4   \n",
       "poec                   0                   9                1   \n",
       "malpegue               0                   0                0   \n",
       "aijue                  1                   0                0   \n",
       "tolhuin                1                   4                3   \n",
       "vallerga               0                   0                0   \n",
       "yarca                  0                   0                0   \n",
       "blv                    0                   0                0   \n",
       "portho                 0                   1                1   \n",
       "\n",
       "          cordoba_ocurrencias  cordoba_usuarios       ...        \\\n",
       "palabra                                               ...         \n",
       "chivil                      0                 0       ...         \n",
       "ush                         5                 3       ...         \n",
       "poec                        1                 1       ...         \n",
       "malpegue                    1                 1       ...         \n",
       "aijue                       0                 0       ...         \n",
       "tolhuin                     3                 1       ...         \n",
       "vallerga                    0                 0       ...         \n",
       "yarca                       2                 1       ...         \n",
       "blv                         0                 0       ...         \n",
       "portho                      0                 0       ...         \n",
       "\n",
       "          guaranitica_usuarios  noroeste_ocurrencias  fnorm_noroeste  \\\n",
       "palabra                                                                \n",
       "chivil                       1                     0        0.000000   \n",
       "ush                         11                    42        0.276578   \n",
       "poec                         0                     0        0.000000   \n",
       "malpegue                     0                     0        0.000000   \n",
       "aijue                      705                     0        0.000000   \n",
       "tolhuin                      2                     5        0.032926   \n",
       "vallerga                     0                     0        0.000000   \n",
       "yarca                        0                     3        0.019756   \n",
       "blv                          3                     6        0.039511   \n",
       "portho                       0                     0        0.000000   \n",
       "\n",
       "          noroeste_usuarios  fnorm_region_max   region_max  fnorm_region_min  \\\n",
       "palabra                                                                        \n",
       "chivil                    0          7.424642      litoral          0.009863   \n",
       "ush                      18         18.695060      litoral          0.082872   \n",
       "poec                      0          4.605158      litoral          0.016530   \n",
       "malpegue                  0         31.491247         cuyo          0.033061   \n",
       "aijue                     0         21.560377  guaranitica          0.016574   \n",
       "tolhuin                   5         11.059897      litoral          0.016574   \n",
       "vallerga                  0          7.262991      litoral          0.016530   \n",
       "yarca                     3         16.408597         cuyo          0.007519   \n",
       "blv                       4         85.561838      central          0.039452   \n",
       "portho                    0         12.530202         cuyo          0.018797   \n",
       "\n",
       "           region_min  region_sin_palabra  max_dif_region  \n",
       "palabra                                                    \n",
       "chivil    guaranitica                   3      752.782165  \n",
       "ush              cuyo                   0      225.590394  \n",
       "poec          central                   2      278.585597  \n",
       "malpegue      central                   2      952.519835  \n",
       "aijue            cuyo                   2     1300.828621  \n",
       "tolhuin          cuyo                   0      667.290306  \n",
       "vallerga      central                   3      439.369284  \n",
       "yarca         litoral                   1     2182.393459  \n",
       "blv       guaranitica                   0     2168.772148  \n",
       "portho        litoral                   3      666.622002  \n",
       "\n",
       "[10 rows x 108 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words = pd.read_csv(\"../output/listados/listado_completo.csv\")\n",
    "df_words.set_index(\"palabra\", inplace=True)\n",
    "df_words.sort_values(\"rank_personas\", ascending=True, inplace=True)\n",
    "\n",
    "df_words.iloc[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos qué performance tiene usando 1000, 2000, 3000, y así..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmperez/.pyenv/versions/3.6.5/envs/contrastes/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 palabras ----> accuracy 60.40\n",
      "750 palabras ----> accuracy 63.12\n",
      "1000 palabras ----> accuracy 65.68\n",
      "1250 palabras ----> accuracy 66.64\n",
      "1500 palabras ----> accuracy 67.92\n",
      "1750 palabras ----> accuracy 69.12\n",
      "2000 palabras ----> accuracy 69.76\n",
      "2250 palabras ----> accuracy 71.20\n",
      "2500 palabras ----> accuracy 71.44\n",
      "2750 palabras ----> accuracy 70.72\n",
      "3000 palabras ----> accuracy 69.76\n",
      "3250 palabras ----> accuracy 69.84\n",
      "3500 palabras ----> accuracy 70.48\n",
      "3750 palabras ----> accuracy 70.80\n",
      "4000 palabras ----> accuracy 69.04\n",
      "4250 palabras ----> accuracy 69.52\n",
      "4500 palabras ----> accuracy 69.52\n",
      "4750 palabras ----> accuracy 69.44\n",
      "5000 palabras ----> accuracy 69.84\n",
      "5250 palabras ----> accuracy 67.60\n",
      "5500 palabras ----> accuracy 68.24\n",
      "5750 palabras ----> accuracy 68.24\n",
      "6000 palabras ----> accuracy 68.00\n",
      "6250 palabras ----> accuracy 68.24\n",
      "6500 palabras ----> accuracy 68.72\n",
      "6750 palabras ----> accuracy 68.88\n",
      "7000 palabras ----> accuracy 69.12\n",
      "7250 palabras ----> accuracy 68.32\n",
      "7500 palabras ----> accuracy 68.40\n",
      "7750 palabras ----> accuracy 68.56\n",
      "8000 palabras ----> accuracy 68.80\n",
      "8250 palabras ----> accuracy 69.36\n",
      "8500 palabras ----> accuracy 69.60\n",
      "8750 palabras ----> accuracy 70.24\n",
      "9000 palabras ----> accuracy 70.96\n",
      "9250 palabras ----> accuracy 71.28\n",
      "9500 palabras ----> accuracy 71.36\n",
      "9750 palabras ----> accuracy 71.28\n",
      "10000 palabras ----> accuracy 71.44\n",
      "10250 palabras ----> accuracy 71.68\n",
      "10500 palabras ----> accuracy 71.52\n",
      "10750 palabras ----> accuracy 71.36\n",
      "11000 palabras ----> accuracy 71.68\n",
      "11250 palabras ----> accuracy 71.84\n",
      "11500 palabras ----> accuracy 72.16\n",
      "11750 palabras ----> accuracy 71.60\n",
      "12000 palabras ----> accuracy 71.60\n",
      "12250 palabras ----> accuracy 72.16\n",
      "12500 palabras ----> accuracy 72.24\n",
      "12750 palabras ----> accuracy 72.40\n",
      "13000 palabras ----> accuracy 70.16\n",
      "13250 palabras ----> accuracy 70.32\n",
      "13500 palabras ----> accuracy 70.24\n",
      "13750 palabras ----> accuracy 70.24\n",
      "14000 palabras ----> accuracy 70.16\n",
      "14250 palabras ----> accuracy 70.32\n",
      "14500 palabras ----> accuracy 70.64\n",
      "14750 palabras ----> accuracy 70.80\n",
      "15000 palabras ----> accuracy 70.72\n",
      "15250 palabras ----> accuracy 70.80\n",
      "15500 palabras ----> accuracy 70.72\n",
      "15750 palabras ----> accuracy 70.96\n",
      "16000 palabras ----> accuracy 71.04\n",
      "16250 palabras ----> accuracy 71.20\n",
      "16500 palabras ----> accuracy 71.04\n",
      "16750 palabras ----> accuracy 71.04\n",
      "17000 palabras ----> accuracy 70.40\n",
      "17250 palabras ----> accuracy 70.80\n",
      "17500 palabras ----> accuracy 70.64\n",
      "17750 palabras ----> accuracy 70.80\n",
      "18000 palabras ----> accuracy 70.80\n",
      "18250 palabras ----> accuracy 70.88\n",
      "18500 palabras ----> accuracy 71.12\n",
      "18750 palabras ----> accuracy 71.04\n",
      "19000 palabras ----> accuracy 70.96\n",
      "19250 palabras ----> accuracy 70.88\n",
      "19500 palabras ----> accuracy 71.12\n",
      "19750 palabras ----> accuracy 71.52\n",
      "20000 palabras ----> accuracy 71.44\n",
      "20250 palabras ----> accuracy 71.44\n",
      "20500 palabras ----> accuracy 71.52\n",
      "20750 palabras ----> accuracy 71.60\n",
      "21000 palabras ----> accuracy 71.52\n",
      "21250 palabras ----> accuracy 71.52\n",
      "21500 palabras ----> accuracy 71.52\n",
      "21750 palabras ----> accuracy 71.60\n",
      "22000 palabras ----> accuracy 71.60\n",
      "22250 palabras ----> accuracy 71.76\n",
      "22500 palabras ----> accuracy 71.68\n",
      "22750 palabras ----> accuracy 71.76\n",
      "23000 palabras ----> accuracy 71.60\n",
      "23250 palabras ----> accuracy 71.52\n",
      "23500 palabras ----> accuracy 71.60\n",
      "23750 palabras ----> accuracy 71.68\n",
      "24000 palabras ----> accuracy 71.60\n",
      "24250 palabras ----> accuracy 71.52\n",
      "24500 palabras ----> accuracy 71.60\n",
      "24750 palabras ----> accuracy 71.44\n",
      "25000 palabras ----> accuracy 71.60\n",
      "25250 palabras ----> accuracy 71.44\n",
      "25500 palabras ----> accuracy 71.76\n",
      "25750 palabras ----> accuracy 71.60\n",
      "26000 palabras ----> accuracy 71.60\n",
      "26250 palabras ----> accuracy 71.76\n",
      "26500 palabras ----> accuracy 71.68\n",
      "26750 palabras ----> accuracy 71.68\n",
      "27000 palabras ----> accuracy 71.76\n",
      "27250 palabras ----> accuracy 71.84\n",
      "27500 palabras ----> accuracy 71.68\n",
      "27750 palabras ----> accuracy 71.84\n",
      "28000 palabras ----> accuracy 71.76\n",
      "28250 palabras ----> accuracy 71.84\n",
      "28500 palabras ----> accuracy 71.68\n",
      "28750 palabras ----> accuracy 66.08\n",
      "29000 palabras ----> accuracy 66.24\n",
      "29250 palabras ----> accuracy 66.24\n",
      "29500 palabras ----> accuracy 66.32\n",
      "29750 palabras ----> accuracy 66.40\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clfs = {}\n",
    "scores = {}\n",
    "\n",
    "for num_words in range(500, 30000, 250):    \n",
    "    liw_vectorizer = CountVectorizer(\n",
    "        tokenizer=tokenizer.tokenize,\n",
    "        vocabulary=df_words.index[:num_words])\n",
    "\n",
    "    X_train = liw_vectorizer.transform(df_train[\"text\"])\n",
    "    X_test = liw_vectorizer.transform(df_test[\"text\"])\n",
    "\n",
    "    clf = LogisticRegression(multi_class='multinomial', solver='saga')\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    scores[num_words] = clf.score(X_test, y_test)\n",
    "    print(\"{} palabras ----> accuracy {:.2f}\".format(num_words, scores[num_words]*100))\n",
    "    clfs[num_words] = clf\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2500 palabras dan un accuracy de 71%. BASTANTE BIEN. Luego disminuye la performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{500: 0.604,\n",
       " 750: 0.6312,\n",
       " 1000: 0.6568,\n",
       " 1250: 0.6664,\n",
       " 1500: 0.6792,\n",
       " 1750: 0.6912,\n",
       " 2000: 0.6976,\n",
       " 2250: 0.712,\n",
       " 2500: 0.7144,\n",
       " 2750: 0.7072,\n",
       " 3000: 0.6976,\n",
       " 3250: 0.6984,\n",
       " 3500: 0.7048,\n",
       " 3750: 0.708,\n",
       " 4000: 0.6904,\n",
       " 4250: 0.6952,\n",
       " 4500: 0.6952,\n",
       " 4750: 0.6944,\n",
       " 5000: 0.6984,\n",
       " 5250: 0.676,\n",
       " 5500: 0.6824,\n",
       " 5750: 0.6824,\n",
       " 6000: 0.68,\n",
       " 6250: 0.6824,\n",
       " 6500: 0.6872,\n",
       " 6750: 0.6888,\n",
       " 7000: 0.6912,\n",
       " 7250: 0.6832,\n",
       " 7500: 0.684,\n",
       " 7750: 0.6856,\n",
       " 8000: 0.688,\n",
       " 8250: 0.6936,\n",
       " 8500: 0.696,\n",
       " 8750: 0.7024,\n",
       " 9000: 0.7096,\n",
       " 9250: 0.7128,\n",
       " 9500: 0.7136,\n",
       " 9750: 0.7128,\n",
       " 10000: 0.7144,\n",
       " 10250: 0.7168,\n",
       " 10500: 0.7152,\n",
       " 10750: 0.7136,\n",
       " 11000: 0.7168,\n",
       " 11250: 0.7184,\n",
       " 11500: 0.7216,\n",
       " 11750: 0.716,\n",
       " 12000: 0.716,\n",
       " 12250: 0.7216,\n",
       " 12500: 0.7224,\n",
       " 12750: 0.724,\n",
       " 13000: 0.7016,\n",
       " 13250: 0.7032,\n",
       " 13500: 0.7024,\n",
       " 13750: 0.7024,\n",
       " 14000: 0.7016,\n",
       " 14250: 0.7032,\n",
       " 14500: 0.7064,\n",
       " 14750: 0.708,\n",
       " 15000: 0.7072,\n",
       " 15250: 0.708,\n",
       " 15500: 0.7072,\n",
       " 15750: 0.7096,\n",
       " 16000: 0.7104,\n",
       " 16250: 0.712,\n",
       " 16500: 0.7104,\n",
       " 16750: 0.7104,\n",
       " 17000: 0.704,\n",
       " 17250: 0.708,\n",
       " 17500: 0.7064,\n",
       " 17750: 0.708,\n",
       " 18000: 0.708,\n",
       " 18250: 0.7088,\n",
       " 18500: 0.7112,\n",
       " 18750: 0.7104,\n",
       " 19000: 0.7096,\n",
       " 19250: 0.7088,\n",
       " 19500: 0.7112,\n",
       " 19750: 0.7152,\n",
       " 20000: 0.7144,\n",
       " 20250: 0.7144,\n",
       " 20500: 0.7152,\n",
       " 20750: 0.716,\n",
       " 21000: 0.7152,\n",
       " 21250: 0.7152,\n",
       " 21500: 0.7152,\n",
       " 21750: 0.716,\n",
       " 22000: 0.716,\n",
       " 22250: 0.7176,\n",
       " 22500: 0.7168,\n",
       " 22750: 0.7176,\n",
       " 23000: 0.716,\n",
       " 23250: 0.7152,\n",
       " 23500: 0.716,\n",
       " 23750: 0.7168,\n",
       " 24000: 0.716,\n",
       " 24250: 0.7152,\n",
       " 24500: 0.716,\n",
       " 24750: 0.7144,\n",
       " 25000: 0.716,\n",
       " 25250: 0.7144,\n",
       " 25500: 0.7176,\n",
       " 25750: 0.716,\n",
       " 26000: 0.716,\n",
       " 26250: 0.7176,\n",
       " 26500: 0.7168,\n",
       " 26750: 0.7168,\n",
       " 27000: 0.7176,\n",
       " 27250: 0.7184,\n",
       " 27500: 0.7168,\n",
       " 27750: 0.7184,\n",
       " 28000: 0.7176,\n",
       " 28250: 0.7184,\n",
       " 28500: 0.7168,\n",
       " 28750: 0.6608,\n",
       " 29000: 0.6624,\n",
       " 29250: 0.6624,\n",
       " 29500: 0.6632,\n",
       " 29750: 0.664}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Con Palabras\n",
    "\n",
    "¿Qué pasa con palabras?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmperez/.pyenv/versions/3.6.5/envs/contrastes/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 palabras ----> accuracy 62.32\n",
      "750 palabras ----> accuracy 67.04\n",
      "1000 palabras ----> accuracy 65.92\n",
      "1250 palabras ----> accuracy 65.68\n",
      "1500 palabras ----> accuracy 66.56\n",
      "1750 palabras ----> accuracy 66.08\n",
      "2000 palabras ----> accuracy 64.72\n",
      "2250 palabras ----> accuracy 65.52\n",
      "2500 palabras ----> accuracy 65.20\n",
      "2750 palabras ----> accuracy 65.20\n",
      "3000 palabras ----> accuracy 65.36\n",
      "3250 palabras ----> accuracy 65.68\n",
      "3500 palabras ----> accuracy 64.64\n",
      "3750 palabras ----> accuracy 65.52\n",
      "4000 palabras ----> accuracy 65.60\n",
      "4250 palabras ----> accuracy 65.68\n",
      "4500 palabras ----> accuracy 66.00\n",
      "4750 palabras ----> accuracy 66.24\n",
      "5000 palabras ----> accuracy 66.40\n",
      "5250 palabras ----> accuracy 66.40\n",
      "5500 palabras ----> accuracy 66.40\n",
      "5750 palabras ----> accuracy 67.04\n",
      "6000 palabras ----> accuracy 66.80\n",
      "6250 palabras ----> accuracy 67.28\n",
      "6500 palabras ----> accuracy 65.76\n",
      "6750 palabras ----> accuracy 65.68\n",
      "7000 palabras ----> accuracy 66.16\n",
      "7250 palabras ----> accuracy 66.56\n",
      "7500 palabras ----> accuracy 66.40\n",
      "7750 palabras ----> accuracy 66.40\n",
      "8000 palabras ----> accuracy 64.56\n",
      "8250 palabras ----> accuracy 64.48\n",
      "8500 palabras ----> accuracy 64.56\n",
      "8750 palabras ----> accuracy 65.76\n",
      "9000 palabras ----> accuracy 65.68\n",
      "9250 palabras ----> accuracy 65.60\n",
      "9500 palabras ----> accuracy 65.68\n",
      "9750 palabras ----> accuracy 65.84\n",
      "10000 palabras ----> accuracy 65.92\n",
      "10250 palabras ----> accuracy 65.68\n",
      "10500 palabras ----> accuracy 65.84\n",
      "10750 palabras ----> accuracy 65.92\n",
      "11000 palabras ----> accuracy 65.20\n",
      "11250 palabras ----> accuracy 65.12\n",
      "11500 palabras ----> accuracy 64.96\n",
      "11750 palabras ----> accuracy 65.84\n",
      "12000 palabras ----> accuracy 65.28\n",
      "12250 palabras ----> accuracy 65.04\n",
      "12500 palabras ----> accuracy 64.96\n",
      "12750 palabras ----> accuracy 64.64\n",
      "13000 palabras ----> accuracy 65.04\n",
      "13250 palabras ----> accuracy 65.20\n",
      "13500 palabras ----> accuracy 65.36\n",
      "13750 palabras ----> accuracy 65.28\n",
      "14000 palabras ----> accuracy 65.20\n",
      "14250 palabras ----> accuracy 65.20\n",
      "14500 palabras ----> accuracy 65.20\n",
      "14750 palabras ----> accuracy 65.28\n",
      "15000 palabras ----> accuracy 65.60\n",
      "15250 palabras ----> accuracy 65.84\n",
      "15500 palabras ----> accuracy 65.92\n",
      "15750 palabras ----> accuracy 66.00\n",
      "16000 palabras ----> accuracy 65.76\n",
      "16250 palabras ----> accuracy 65.92\n",
      "16500 palabras ----> accuracy 65.76\n",
      "16750 palabras ----> accuracy 66.00\n",
      "17000 palabras ----> accuracy 65.92\n",
      "17250 palabras ----> accuracy 65.92\n",
      "17500 palabras ----> accuracy 65.84\n",
      "17750 palabras ----> accuracy 56.08\n",
      "18000 palabras ----> accuracy 56.16\n",
      "18250 palabras ----> accuracy 56.08\n",
      "18500 palabras ----> accuracy 56.64\n",
      "18750 palabras ----> accuracy 56.64\n",
      "19000 palabras ----> accuracy 56.40\n",
      "19250 palabras ----> accuracy 56.64\n",
      "19500 palabras ----> accuracy 56.48\n",
      "19750 palabras ----> accuracy 56.64\n",
      "20000 palabras ----> accuracy 56.96\n",
      "20250 palabras ----> accuracy 56.48\n",
      "20500 palabras ----> accuracy 56.56\n",
      "20750 palabras ----> accuracy 56.88\n",
      "21000 palabras ----> accuracy 56.72\n",
      "21250 palabras ----> accuracy 56.64\n",
      "21500 palabras ----> accuracy 56.56\n",
      "21750 palabras ----> accuracy 56.96\n",
      "22000 palabras ----> accuracy 57.68\n",
      "22250 palabras ----> accuracy 58.32\n",
      "22500 palabras ----> accuracy 58.16\n",
      "22750 palabras ----> accuracy 57.76\n",
      "23000 palabras ----> accuracy 58.16\n",
      "23250 palabras ----> accuracy 57.76\n",
      "23500 palabras ----> accuracy 57.52\n",
      "23750 palabras ----> accuracy 57.68\n",
      "24000 palabras ----> accuracy 57.20\n",
      "24250 palabras ----> accuracy 57.60\n",
      "24500 palabras ----> accuracy 57.60\n",
      "24750 palabras ----> accuracy 57.68\n",
      "25000 palabras ----> accuracy 57.84\n",
      "25250 palabras ----> accuracy 56.32\n",
      "25500 palabras ----> accuracy 55.76\n",
      "25750 palabras ----> accuracy 55.84\n",
      "26000 palabras ----> accuracy 55.84\n",
      "26250 palabras ----> accuracy 55.60\n",
      "26500 palabras ----> accuracy 56.08\n",
      "26750 palabras ----> accuracy 55.84\n",
      "27000 palabras ----> accuracy 55.76\n",
      "27250 palabras ----> accuracy 55.84\n",
      "27500 palabras ----> accuracy 55.68\n",
      "27750 palabras ----> accuracy 56.08\n",
      "28000 palabras ----> accuracy 55.84\n",
      "28250 palabras ----> accuracy 56.16\n",
      "28500 palabras ----> accuracy 56.16\n",
      "28750 palabras ----> accuracy 56.32\n",
      "29000 palabras ----> accuracy 56.08\n",
      "29250 palabras ----> accuracy 57.20\n",
      "29500 palabras ----> accuracy 57.12\n",
      "29750 palabras ----> accuracy 56.24\n"
     ]
    }
   ],
   "source": [
    "df_words.sort_values(\"rank_palabras\", ascending=True, inplace=True)\n",
    "\n",
    "clfs_palabras = {}\n",
    "scores_palabras = {}\n",
    "\n",
    "for num_words in range(500, 30000, 250):    \n",
    "    liw_vectorizer = CountVectorizer(\n",
    "        tokenizer=tokenizer.tokenize,\n",
    "        vocabulary=df_words.index[:num_words])\n",
    "\n",
    "    X_train = liw_vectorizer.transform(df_train[\"text\"])\n",
    "    X_test = liw_vectorizer.transform(df_test[\"text\"])\n",
    "\n",
    "    clf = LogisticRegression(multi_class='multinomial', solver='saga')\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    scores_palabras[num_words] = clf.score(X_test, y_test)\n",
    "    print(\"{} palabras ----> accuracy {:.2f}\".format(num_words, scores_palabras[num_words]*100))\n",
    "    clfs_palabras[num_words] = clf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{500: 0.6232,\n",
       " 750: 0.6704,\n",
       " 1000: 0.6592,\n",
       " 1250: 0.6568,\n",
       " 1500: 0.6656,\n",
       " 1750: 0.6608,\n",
       " 2000: 0.6472,\n",
       " 2250: 0.6552,\n",
       " 2500: 0.652,\n",
       " 2750: 0.652,\n",
       " 3000: 0.6536,\n",
       " 3250: 0.6568,\n",
       " 3500: 0.6464,\n",
       " 3750: 0.6552,\n",
       " 4000: 0.656,\n",
       " 4250: 0.6568,\n",
       " 4500: 0.66,\n",
       " 4750: 0.6624,\n",
       " 5000: 0.664,\n",
       " 5250: 0.664,\n",
       " 5500: 0.664,\n",
       " 5750: 0.6704,\n",
       " 6000: 0.668,\n",
       " 6250: 0.6728,\n",
       " 6500: 0.6576,\n",
       " 6750: 0.6568,\n",
       " 7000: 0.6616,\n",
       " 7250: 0.6656,\n",
       " 7500: 0.664,\n",
       " 7750: 0.664,\n",
       " 8000: 0.6456,\n",
       " 8250: 0.6448,\n",
       " 8500: 0.6456,\n",
       " 8750: 0.6576,\n",
       " 9000: 0.6568,\n",
       " 9250: 0.656,\n",
       " 9500: 0.6568,\n",
       " 9750: 0.6584,\n",
       " 10000: 0.6592,\n",
       " 10250: 0.6568,\n",
       " 10500: 0.6584,\n",
       " 10750: 0.6592,\n",
       " 11000: 0.652,\n",
       " 11250: 0.6512,\n",
       " 11500: 0.6496,\n",
       " 11750: 0.6584,\n",
       " 12000: 0.6528,\n",
       " 12250: 0.6504,\n",
       " 12500: 0.6496,\n",
       " 12750: 0.6464,\n",
       " 13000: 0.6504,\n",
       " 13250: 0.652,\n",
       " 13500: 0.6536,\n",
       " 13750: 0.6528,\n",
       " 14000: 0.652,\n",
       " 14250: 0.652,\n",
       " 14500: 0.652,\n",
       " 14750: 0.6528,\n",
       " 15000: 0.656,\n",
       " 15250: 0.6584,\n",
       " 15500: 0.6592,\n",
       " 15750: 0.66,\n",
       " 16000: 0.6576,\n",
       " 16250: 0.6592,\n",
       " 16500: 0.6576,\n",
       " 16750: 0.66,\n",
       " 17000: 0.6592,\n",
       " 17250: 0.6592,\n",
       " 17500: 0.6584,\n",
       " 17750: 0.5608,\n",
       " 18000: 0.5616,\n",
       " 18250: 0.5608,\n",
       " 18500: 0.5664,\n",
       " 18750: 0.5664,\n",
       " 19000: 0.564,\n",
       " 19250: 0.5664,\n",
       " 19500: 0.5648,\n",
       " 19750: 0.5664,\n",
       " 20000: 0.5696,\n",
       " 20250: 0.5648,\n",
       " 20500: 0.5656,\n",
       " 20750: 0.5688,\n",
       " 21000: 0.5672,\n",
       " 21250: 0.5664,\n",
       " 21500: 0.5656,\n",
       " 21750: 0.5696,\n",
       " 22000: 0.5768,\n",
       " 22250: 0.5832,\n",
       " 22500: 0.5816,\n",
       " 22750: 0.5776,\n",
       " 23000: 0.5816,\n",
       " 23250: 0.5776,\n",
       " 23500: 0.5752,\n",
       " 23750: 0.5768,\n",
       " 24000: 0.572,\n",
       " 24250: 0.576,\n",
       " 24500: 0.576,\n",
       " 24750: 0.5768,\n",
       " 25000: 0.5784,\n",
       " 25250: 0.5632,\n",
       " 25500: 0.5576,\n",
       " 25750: 0.5584,\n",
       " 26000: 0.5584,\n",
       " 26250: 0.556,\n",
       " 26500: 0.5608,\n",
       " 26750: 0.5584,\n",
       " 27000: 0.5576,\n",
       " 27250: 0.5584,\n",
       " 27500: 0.5568,\n",
       " 27750: 0.5608,\n",
       " 28000: 0.5584,\n",
       " 28250: 0.5616,\n",
       " 28500: 0.5616,\n",
       " 28750: 0.5632,\n",
       " 29000: 0.5608,\n",
       " 29250: 0.572,\n",
       " 29500: 0.5712,\n",
       " 29750: 0.5624}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_palabras"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
