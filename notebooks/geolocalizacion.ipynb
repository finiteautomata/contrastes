{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geolocalización\n",
    "\n",
    "En esta notebook haremos un intento de geolocalización con los textos de los usuarios..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient('localhost')\n",
    "\n",
    "db = client['contrastes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('5ba53c9827a5141aaa383eb9'),\n",
       " 'created_at': 'Tue Nov 05 14:48:51 +0000 2013',\n",
       " 'id': 397737276736040960,\n",
       " 'place': None,\n",
       " 'provincia': 'larioja',\n",
       " 'text': 'Estoy tan asustada :(',\n",
       " 'tokens': ['estoy', 'tan', 'asustada'],\n",
       " 'user_id': 301800629}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.tweets.find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tenemos 56308 usuarios\n"
     ]
    }
   ],
   "source": [
    "user_ids = list(db.users.distinct('id'))\n",
    "print(\"Tenemos {} usuarios\".format(len(user_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hagamos lo siguiente:\n",
    "\n",
    "- Entrenemos con unigramas una regresión logística para \n",
    "- Luego probemos con los regionalismos\n",
    "\n",
    "Primero, partamos en train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>provincia</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>buenosaires</th>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>catamarca</th>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chaco</th>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chubut</th>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cordoba</th>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>corrientes</th>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entrerios</th>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>formosa</th>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jujuy</th>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lapampa</th>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>larioja</th>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mendoza</th>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>misiones</th>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neuquen</th>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rionegro</th>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salta</th>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sanjuan</th>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sanluis</th>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>santacruz</th>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>santafe</th>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>santiago</th>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tierradelfuego</th>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tucuman</th>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                text\n",
       "provincia           \n",
       "buenosaires      165\n",
       "catamarca        165\n",
       "chaco            162\n",
       "chubut           145\n",
       "cordoba          167\n",
       "corrientes       182\n",
       "entrerios        174\n",
       "formosa          161\n",
       "jujuy            155\n",
       "lapampa          149\n",
       "larioja          163\n",
       "mendoza          144\n",
       "misiones         163\n",
       "neuquen          169\n",
       "rionegro         154\n",
       "salta            153\n",
       "sanjuan          170\n",
       "sanluis          161\n",
       "santacruz        171\n",
       "santafe          170\n",
       "santiago         170\n",
       "tierradelfuego   174\n",
       "tucuman          163"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(2019)\n",
    "\n",
    "sample_user_ids = random.sample(user_ids, 5000)\n",
    "\n",
    "# Nos quedamos sólo con los campos que nos interesan\n",
    "users = list(db.users.find({\"id\": {\"$in\": sample_user_ids}}, {\"id\": 1, \"_id\": 0, \"text\": 1, \"provincia\": 1}))\n",
    "\n",
    "\n",
    "\n",
    "train_users, test_users = train_test_split(users)\n",
    "\n",
    "df_train = pd.DataFrame(train_users)\n",
    "df_train.set_index(\"id\", inplace=True)\n",
    "\n",
    "\n",
    "df_test = pd.DataFrame(test_users)\n",
    "df_test.set_index(\"id\", inplace=True)\n",
    "\n",
    "df_train.groupby(\"provincia\").count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Palabras precalculadas\n",
    "\n",
    "Carguemos antes las palabras que sabemos que ocurren una cantidad razonable de veces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.15, max_features=None, min_df=0.0001,\n",
       "        ngram_range=(1, 2), preprocessor=None,\n",
       "        stop_words=['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al', 'lo', 'como', 'más', 'pero', 'sus', 'le', 'ya', 'o', 'este', 'sí', 'porque', 'esta', 'entre', 'cuando', 'muy', 'sin', 'sobre', 'también', 'me', 'hasta', 'hay', 'donde'...tuviésemos', 'tuvieseis', 'tuviesen', 'teniendo', 'tenido', 'tenida', 'tenidos', 'tenidas', 'tened'],\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7fe7d652b160>>,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "vectorizer = CountVectorizer(\n",
    "    tokenizer=tokenizer.tokenize, stop_words=stopwords.words('spanish'),\n",
    "    min_df=0.0001, max_df=0.15, ngram_range=(1, 2),\n",
    ")\n",
    "\n",
    "vectorizer.fit(df_train[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario del vectorizador: 12957624 palabras\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulario del vectorizador: {} palabras\".format(len(vectorizer.vocabulary_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer.transform(df_train[\"text\"])\n",
    "X_test = vectorizer.transform(df_test[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "province_encoder = LabelEncoder()\n",
    "\n",
    "province_encoder.fit(df_train[\"provincia\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = province_encoder.transform(df_train[\"provincia\"].values)\n",
    "y_test = province_encoder.transform(df_test[\"provincia\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La reg. logística será un softmax, así que elijo `multi_class='multinomial'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(multi_class='multinomial', solver='saga')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "32% de accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando sólo \"regionalismos\" o LIW (Location Indicative Words)\n",
    "\n",
    "Usemos ahora nuestros \"features\". Es decir, probemos con porcentajes de las palabras encontradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words = pd.read_csv(\"../output/listados/listado_completo.csv\")\n",
    "df_words.set_index(\"palabra\", inplace=True)\n",
    "df_words.sort_values(\"rank_personas\", ascending=True, inplace=True)\n",
    "\n",
    "df_words.iloc[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos qué performance tiene usando 1000, 2000, 3000, y así..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clfs = {}\n",
    "scores = {}\n",
    "\n",
    "for num_words in range(250, 30000, 250):    \n",
    "    liw_vectorizer = CountVectorizer(\n",
    "        tokenizer=tokenizer.tokenize,\n",
    "        vocabulary=df_words.index[:num_words])\n",
    "\n",
    "    X_train = liw_vectorizer.transform(df_train[\"text\"])\n",
    "    X_test = liw_vectorizer.transform(df_test[\"text\"])\n",
    "\n",
    "    clf = LogisticRegression(multi_class='multinomial', solver='saga')\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    scores[num_words] = clf.score(X_test, y_test)\n",
    "    clfs[num_words] = clf\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto me da miedo..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
