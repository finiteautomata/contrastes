{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geolocalización\n",
    "\n",
    "En esta notebook haremos un intento de geolocalización con los textos de los usuarios.\n",
    "\n",
    "Pero haremos algo distinto: usaremos Term Frequency - Inverse Province Frequency (TF-IPF)\n",
    "\n",
    "\n",
    "[Geolocation prediction in social media data by finding location indicative words](http://www.aclweb.org/anthology/C12-1064)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient('localhost', 27018)\n",
    "\n",
    "db = client['contrastes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_json(\"data/geoloc/users_train.json\")\n",
    "df_test = pd.read_json(\"data/geoloc/users_test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hagamos lo siguiente:\n",
    "\n",
    "- Entrenemos con unigramas una regresión logística para \n",
    "- Luego probemos con los regionalismos\n",
    "\n",
    "Primero, partamos en train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>provincia</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>buenosaires</th>\n",
       "      <td>337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>catamarca</th>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chaco</th>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chubut</th>\n",
       "      <td>328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cordoba</th>\n",
       "      <td>317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>corrientes</th>\n",
       "      <td>345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entrerios</th>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>formosa</th>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jujuy</th>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lapampa</th>\n",
       "      <td>324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>larioja</th>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mendoza</th>\n",
       "      <td>326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>misiones</th>\n",
       "      <td>327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neuquen</th>\n",
       "      <td>350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rionegro</th>\n",
       "      <td>313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salta</th>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sanjuan</th>\n",
       "      <td>326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sanluis</th>\n",
       "      <td>321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>santacruz</th>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>santafe</th>\n",
       "      <td>347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>santiago</th>\n",
       "      <td>294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tierradelfuego</th>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tucuman</th>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                text\n",
       "provincia           \n",
       "buenosaires      337\n",
       "catamarca        341\n",
       "chaco            331\n",
       "chubut           328\n",
       "cordoba          317\n",
       "corrientes       345\n",
       "entrerios        338\n",
       "formosa          286\n",
       "jujuy            339\n",
       "lapampa          324\n",
       "larioja          306\n",
       "mendoza          326\n",
       "misiones         327\n",
       "neuquen          350\n",
       "rionegro         313\n",
       "salta            346\n",
       "sanjuan          326\n",
       "sanluis          321\n",
       "santacruz        288\n",
       "santafe          347\n",
       "santiago         294\n",
       "tierradelfuego   332\n",
       "tucuman          338"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.groupby(\"provincia\").count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Palabras precalculadas\n",
    "\n",
    "Carguemos antes las palabras que sabemos que ocurren una cantidad razonable de veces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 3s, sys: 544 ms, total: 8min 3s\n",
      "Wall time: 8min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from contrastes.processing import build_dataframe_from_users\n",
    "\n",
    "word_df = build_dataframe_from_users(row for index, row in df_train.iterrows())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmperez/projects/contrastes/notebooks/contrastes/processing.py:185: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  df.columnas_palabras = cant_palabras\n",
      "/home/jmperez/projects/contrastes/notebooks/contrastes/processing.py:186: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  df.columnas_personas = cant_personas\n"
     ]
    }
   ],
   "source": [
    "from contrastes.processing import preprocess_raw_df\n",
    "\n",
    "word_df = preprocess_raw_df(word_df, filter_words=(10, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 27.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario del vectorizador: 103783 palabras\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulario del vectorizador: {} palabras\".format(len(vectorizer.vocabulary_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "province_encoder = LabelEncoder()\n",
    "\n",
    "province_encoder.fit(df_train[\"provincia\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = province_encoder.transform(df_train[\"provincia\"].values)\n",
    "y_test = province_encoder.transform(df_test[\"provincia\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La reg. logística será un softmax, así que elijo `multi_class='multinomial'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "38% de accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando sólo \"regionalismos\" o LIW (Location Indicative Words)\n",
    "\n",
    "Usemos ahora nuestros \"features\". Es decir, probemos con porcentajes de las palabras encontradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['buenosaires_ocurrencias', 'buenosaires_usuarios',\n",
       "       'catamarca_ocurrencias', 'catamarca_usuarios', 'chaco_ocurrencias',\n",
       "       'chaco_usuarios', 'chubut_ocurrencias', 'chubut_usuarios',\n",
       "       'cordoba_ocurrencias', 'cordoba_usuarios', 'corrientes_ocurrencias',\n",
       "       'corrientes_usuarios', 'entrerios_ocurrencias', 'entrerios_usuarios',\n",
       "       'formosa_ocurrencias', 'formosa_usuarios', 'jujuy_ocurrencias',\n",
       "       'jujuy_usuarios', 'lapampa_ocurrencias', 'lapampa_usuarios',\n",
       "       'larioja_ocurrencias', 'larioja_usuarios', 'mendoza_ocurrencias',\n",
       "       'mendoza_usuarios', 'misiones_ocurrencias', 'misiones_usuarios',\n",
       "       'neuquen_ocurrencias', 'neuquen_usuarios', 'rionegro_ocurrencias',\n",
       "       'rionegro_usuarios', 'salta_ocurrencias', 'salta_usuarios',\n",
       "       'sanjuan_ocurrencias', 'sanjuan_usuarios', 'sanluis_ocurrencias',\n",
       "       'sanluis_usuarios', 'santacruz_ocurrencias', 'santacruz_usuarios',\n",
       "       'santafe_ocurrencias', 'santafe_usuarios', 'santiago_ocurrencias',\n",
       "       'santiago_usuarios', 'tierradelfuego_ocurrencias',\n",
       "       'tierradelfuego_usuarios', 'tucuman_ocurrencias', 'tucuman_usuarios',\n",
       "       'cant_provincias', 'cant_palabra', 'cant_usuarios', 'ipf'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from contrastes.lists import add_ival\n",
    "\n",
    "word_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cant_palabra</th>\n",
       "      <th>cant_provincias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tiemposur</th>\n",
       "      <td>883.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logroño</th>\n",
       "      <td>711.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nihuil</th>\n",
       "      <td>450.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chivil</th>\n",
       "      <td>332.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ipauss</th>\n",
       "      <td>315.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vallerga</th>\n",
       "      <td>291.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asprodema</th>\n",
       "      <td>290.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cdelu</th>\n",
       "      <td>244.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>calahorra</th>\n",
       "      <td>216.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>canicross</th>\n",
       "      <td>202.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           cant_palabra  cant_provincias\n",
       "tiemposur         883.0                1\n",
       "logroño           711.0                1\n",
       "nihuil            450.0                1\n",
       "chivil            332.0                1\n",
       "ipauss            315.0                1\n",
       "vallerga          291.0                1\n",
       "asprodema         290.0                1\n",
       "cdelu             244.0                1\n",
       "calahorra         216.0                1\n",
       "canicross         202.0                1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_df.sort_values([\"cant_provincias\", \"cant_palabra\"], ascending=[True, False], inplace=True)\n",
    "\n",
    "word_df.iloc[:10][[\"cant_palabra\", \"cant_provincias\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos qué performance tiene usando 1000, 2000, 3000, y así..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing\n",
      "CPU times: user 10min 3s, sys: 448 ms, total: 10min 3s\n",
      "Wall time: 10min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "liw_vectorizer = CountVectorizer(\n",
    "    tokenizer=tokenizer.tokenize,\n",
    "    vocabulary=word_df.index)\n",
    "\n",
    "X_train = liw_vectorizer.fit_transform(df_train[\"text\"])\n",
    "print(\"Vectorizing\")\n",
    "X_test = liw_vectorizer.transform(df_test[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya las tenemos vectorizadas en el orden esperado!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clfs = {}\n",
    "scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 palabras ----> accuracy 21.88\n",
      "500 palabras ----> accuracy 27.48\n",
      "750 palabras ----> accuracy 30.00\n",
      "1000 palabras ----> accuracy 31.56\n",
      "1250 palabras ----> accuracy 36.76\n",
      "1500 palabras ----> accuracy 41.80\n",
      "1750 palabras ----> accuracy 43.28\n",
      "2000 palabras ----> accuracy 43.84\n",
      "2250 palabras ----> accuracy 44.28\n",
      "2500 palabras ----> accuracy 45.12\n",
      "2750 palabras ----> accuracy 49.16\n",
      "3000 palabras ----> accuracy 52.48\n",
      "3250 palabras ----> accuracy 53.76\n",
      "3500 palabras ----> accuracy 53.84\n",
      "3750 palabras ----> accuracy 54.28\n",
      "4000 palabras ----> accuracy 54.24\n",
      "4250 palabras ----> accuracy 56.76\n",
      "4500 palabras ----> accuracy 57.56\n",
      "4750 palabras ----> accuracy 58.16\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "for num_words in range(250, 5000, 250):\n",
    "    if num_words in clfs:\n",
    "        print(\"{} palabras ----> accuracy {:.2f}\".format(num_words, scores[num_words]*100))\n",
    "        continue\n",
    "    X_tr = X_train[:, :num_words].todense()\n",
    "    X_tst = X_test[:, :num_words].todense()\n",
    "    \n",
    "    clf = LogisticRegression(\n",
    "        multi_class='multinomial', solver='saga', penalty='l2', \n",
    "        max_iter=200, n_jobs=-1)\n",
    "    clf.fit(X_tr, y_train)\n",
    "    \n",
    "    scores[num_words] = clf.score(X_tst, y_test)\n",
    "    print(\"{} palabras ----> accuracy {:.2f}\".format(num_words, scores[num_words]*100))\n",
    "    clfs[num_words] = clf\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2500 palabras dan un accuracy de 71%. BASTANTE BIEN. Luego disminuye la performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 palabras ----> accuracy 58.04\n",
      "5500 palabras ----> accuracy 58.24\n",
      "6000 palabras ----> accuracy 60.76\n",
      "6500 palabras ----> accuracy 61.56\n",
      "7000 palabras ----> accuracy 61.60\n",
      "7500 palabras ----> accuracy 61.64\n",
      "8000 palabras ----> accuracy 61.92\n",
      "8500 palabras ----> accuracy 64.08\n",
      "9000 palabras ----> accuracy 64.28\n",
      "9500 palabras ----> accuracy 64.44\n",
      "10000 palabras ----> accuracy 64.36\n",
      "10500 palabras ----> accuracy 64.52\n",
      "11000 palabras ----> accuracy 64.60\n",
      "11500 palabras ----> accuracy 65.60\n",
      "12000 palabras ----> accuracy 65.84\n",
      "12500 palabras ----> accuracy 66.12\n",
      "13000 palabras ----> accuracy 65.96\n",
      "13500 palabras ----> accuracy 65.92\n",
      "14000 palabras ----> accuracy 66.00\n",
      "14500 palabras ----> accuracy 66.04\n",
      "15000 palabras ----> accuracy 66.08\n",
      "15500 palabras ----> accuracy 66.04\n",
      "16000 palabras ----> accuracy 66.08\n",
      "16500 palabras ----> accuracy 67.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmperez/.pyenv/versions/3.6.5/envs/contrastes/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000 palabras ----> accuracy 67.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmperez/.pyenv/versions/3.6.5/envs/contrastes/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17500 palabras ----> accuracy 67.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmperez/.pyenv/versions/3.6.5/envs/contrastes/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000 palabras ----> accuracy 67.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmperez/.pyenv/versions/3.6.5/envs/contrastes/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18500 palabras ----> accuracy 66.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmperez/.pyenv/versions/3.6.5/envs/contrastes/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19000 palabras ----> accuracy 67.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmperez/.pyenv/versions/3.6.5/envs/contrastes/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19500 palabras ----> accuracy 67.08\n"
     ]
    }
   ],
   "source": [
    "for num_words in range(5000, 20000, 500):\n",
    "    if num_words in clfs:\n",
    "        print(\"{} palabras ----> accuracy {:.2f}\".format(num_words, scores[num_words]*100))\n",
    "        continue\n",
    "    X_tr = X_train[:, :num_words].todense()\n",
    "    X_tst = X_test[:, :num_words].todense()\n",
    "    \n",
    "    clf = LogisticRegression(\n",
    "        multi_class='multinomial', solver='saga', penalty='l2', \n",
    "        max_iter=200, n_jobs=-1)\n",
    "    clf.fit(X_tr, y_train)\n",
    "    \n",
    "    scores[num_words] = clf.score(X_tst, y_test)\n",
    "    print(\"{} palabras ----> accuracy {:.2f}\".format(num_words, scores[num_words]*100))\n",
    "    clfs[num_words] = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(clfs, open(\"clfs_ipf.pkl\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
