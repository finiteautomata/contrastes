In the present multidisciplinary work we developed metrics to detect regionalisms, using provinces as unit of measure, and collecting data from Twitter. These metrics used both the count of occurrences of a word as well as the number of users of it, and were based on entropy to measure their ``concentration''.


The metric we create uses entropy to measure the variation in the number of occurrences and the number of users that used it in the different provinces of the country. The 5000 words with the highest contrastivity value were selected to carry out a linguistic validation by the Argentine Academy of Letters. The validation yielded a result with around 300 words worthy of study of those 5000 words, that is, 1 word every 17.
Although there are no other projects that provide a comparison term to assess the degree of success involved in this relationship, there is no doubt that, at least in the detection of local colloquialisms currently in use, the tool poses a real point of inflection for contrastive lexicography.
Several of the words detected from the developed metric will be added to the Speech Dictionary of the Argentines.

Regarding statistical validation, we leave as future work the calculation of a statistical analysis applicable to our metric, since it is outside the scope of this thesis. However, based on the analysis made through the Welch t test, we also have indications of the virtues of the developed metric.

In this paper, the regions formed with a province as a regional unit are analyzed, but this can be changed to replicate the analysis with different granularity. This way you could see the contrastive words in the different Spanish-speaking countries and compare the variations between larger regions or replicate the work within a single province or city.

One of the challenges that this work triggers is that of being able to identify regions / clusters with different dialectal uses. At the same time, it would allow to validate the validity of the regions proposed by Vidal de Battini in 1964 \ cite {vidal1964espanol}.

Also, the normalization process could be improved to have a greater precision in the words used. From a better normalization and the lemmatization with metalinguistic information of the corpus, we could go beyond the lexicon to study the syntactic phenomena of Spanish, as its variation in different regions. Continuing with the research line, we could analyze the lexical contrastivity by comparing the distribution of n-grams. On the other hand, it would be useful to add a system of recognition of names of entities to highlight certain proper names, in such a way that the list of words has more alerts about terms without linguistic interest.

It is important to note the advantages of \ textit {Twitter} since it allowed us to collect a large volume of text data, written by different people with information about their location. Regarding the disadvantages of this platform we can highlight the orthographic errors of the texts, the intentional modification of the words to generate emphasis or with a reason for faster writing. All this leads to an increase in the difficulty to normalize the text. We believe, despite all this, that the volume of data prevails when deciding a platform to collect them.
% En cuanto a las desventajas de esta plataforma podemos destacar los errores ortográficos de los textos. Sin embargo, la modificación intencionada de las palabras con fines expresivos o debido a que, además de la limitación de espacio, el usuario tiende a practicar una escritura más rápida. Con todo, es indudable que esto conlleva a un aumento de la dificultad para normalizar el texto. Creemos, a pesar de todo esto, que el volumen de datos prevalece a la hora de decidir una plataforma para recolectarlos.
